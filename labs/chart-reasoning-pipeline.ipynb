{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16293154-8947-407b-82b2-d440e0426c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e00a487-7f5e-41e4-a2a3-f6320a6abe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "APP_ROOT = os.path.abspath('..')\n",
    "sys.path.append(os.path.abspath(APP_ROOT))\n",
    "\n",
    "from chart_reasoning.pipeline import ChartReasoningPipeline\n",
    "\n",
    "vistext_data_dir = os.path.join(APP_ROOT, 'data', 'vistext-data')\n",
    "output_dir = os.path.join(APP_ROOT, 'output', 'chart-reasoning-trail-run')\n",
    "\n",
    "pipeline = ChartReasoningPipeline(vistext_data_dir, output_dir, \n",
    "                                  chart_type_list=['unaligned_rule', 'color'])\n",
    "# note that `table` type needs executable Chrome in the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e67b1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8822/8822 [01:07<00:00, 130.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# prepare the vistext data.\n",
    "pipeline.refine_vistext_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af73275a-6db5-4b24-b577-435162d85a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  5.99it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):  # increase the number of loop for error skipping cases.\n",
    "    pipeline.task_generation(sample_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "694f16b6-0959-43b0-81d6-4cafe40e6752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.82s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):  # increase the number of loop for error skipping cases.\n",
    "    pipeline.visual_chart_reasoning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9f44bd-4251-4677-8831-c88d22286b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 543.34it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):  # increase the number of loop for error skipping cases.\n",
    "    pipeline.grade_with_text_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6e2f19-82c2-4f73-8976-f66716253bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the graded results to markdown file for easy viewing.\n",
    "pipeline.graded_output_to_md()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0b30d",
   "metadata": {},
   "source": [
    "# Evaluate GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdafbf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "# code_graded_dir = os.path.join(output_dir, '06-code-assistant-grading-output')\n",
    "text_graded_dir = os.path.join(output_dir, '04-text-grading-output')\n",
    "\n",
    "# code_graded_task_ids = [fname.split(\".\")[0] for fname in os.listdir(code_graded_dir) if fname.endswith('.grade.json')]\n",
    "# graded_task_ids = [fname.split(\".\")[0] for fname in os.listdir(text_graded_dir) if fname.endswith('.grade.json')]\n",
    "\n",
    "graded_task_ids = open(\"reported_ids_1000.txt\", \"r\").readlines()\n",
    "graded_task_ids = [one_id.strip() for one_id in graded_task_ids]\n",
    "\n",
    "# RQ2 exp1\n",
    "chart_types = ['line', 'scatter', 'bar']\n",
    "# RQ2 exp2\n",
    "# chart_types = ['pie', 'table', 'bar_anno', 'line_anno', 'scatter_anno']\n",
    "\n",
    "# RQ1 exp1\n",
    "# chart_types = ['unaligned_rule', 'color', 'size', 'scatter']\n",
    "# RQ1 exp2\n",
    "# chart_types = ['rule', 'scatter_size', 'bar', 'bar_color']\n",
    "\n",
    "\n",
    "text_student_answer_correctness_dict_by_chart_type = dict()\n",
    "text_student_judgement_list = dict()\n",
    "for chart_type in chart_types:\n",
    "    text_student_answer_correctness_dict_by_chart_type[chart_type] = dict()\n",
    "    text_student_judgement_list[chart_type] = []\n",
    "\n",
    "\n",
    "\n",
    "valid_task_ids = list(set(graded_task_ids))\n",
    "for task_id in tqdm(valid_task_ids):\n",
    "    # text_graded_task_file = os.path.join(text_graded_dir, f'{task_id}.grade.json')\n",
    "    for chart_type in chart_types:\n",
    "        text_graded_task_file = os.path.join(text_graded_dir, f'{task_id}.{chart_type}.grade.json')\n",
    "\n",
    "        if os.path.exists(text_graded_task_file):\n",
    "            with open(text_graded_task_file, 'r') as f:\n",
    "                text_graded_task = json.load(f)\n",
    "                for question_index, question_dict in enumerate(text_graded_task):\n",
    "\n",
    "                    text_student_judgement_list[chart_type].append(question_dict['student_answer_correctness'].lower())\n",
    "                    if question_dict['student_answer_correctness'] not in text_student_answer_correctness_dict_by_chart_type[chart_type].keys():\n",
    "                        text_student_answer_correctness_dict_by_chart_type[chart_type][question_dict['student_answer_correctness'].lower()] = []\n",
    "                    else:\n",
    "                        text_student_answer_correctness_dict_by_chart_type[chart_type][question_dict['student_answer_correctness'].lower()].append((question_dict['task_id']+\"_\"+str(question_index), question_dict['task_type']))\n",
    "\n",
    "        else:\n",
    "            print(f\"File not found: {text_graded_task_file}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a421152",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_list = []\n",
    "for chart_type in chart_types:\n",
    "    print(f\"Chart Type: {chart_type}\")\n",
    "    print(\"Total number of questions: \", len(text_student_judgement_list[chart_type]))\n",
    "    print(\"Correctness Distribution:\")\n",
    "    total = sum([len(id_list) for id_list in text_student_answer_correctness_dict_by_chart_type[chart_type].values()])\n",
    "    for key, id_list in text_student_answer_correctness_dict_by_chart_type[chart_type].items():\n",
    "        print(key, \":\", len(id_list), f\"({round(len(id_list)/total * 100, 2)}%)\")\n",
    "        if key == 'correct':\n",
    "            correct_list.append(round(len(id_list)/total * 100, 2))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "print(\"Correctness:\", correct_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac6f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "task_types = ['Find Anomalies', 'Find Correlation', 'Determine Range', 'Order', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Retrieve Value', 'Find Clusters', 'Characterize Distribution']\n",
    "\n",
    "for task_type in task_types:\n",
    "    print(f\"Task Type: {task_type}\")\n",
    "    # only consider the correctness of the task type\n",
    "    for chart_type in chart_types:\n",
    "        # print(f\"Chart Type: {chart_type}\")\n",
    "        # get all examples for this task type for this chart and calculate the correctness\n",
    "        # total = len(text_student_answer_correctness_dict_by_chart_type[chart_type][task_type])\n",
    "        correct_cnt = 0\n",
    "        for example in text_student_answer_correctness_dict_by_chart_type[chart_type]['correct']:\n",
    "            if example[1] == task_type:\n",
    "                correct_cnt += 1\n",
    "        # count all\n",
    "        all_example_cnt = 0\n",
    "        for score_type, example_list in text_student_answer_correctness_dict_by_chart_type[chart_type].items():\n",
    "            for example in example_list:\n",
    "                if example[1] == task_type:\n",
    "                    all_example_cnt += 1\n",
    "        \n",
    "        # print(\"Correctness:\", correct_cnt, \"All examples:\", all_example_cnt, f\"({round(correct_cnt/all_example_cnt * 100, 2)}%)\")\n",
    "        print(f\"{round(correct_cnt/all_example_cnt * 100, 2)}\", end=';')\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
