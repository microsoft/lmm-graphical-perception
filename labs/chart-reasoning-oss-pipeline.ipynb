{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16293154-8947-407b-82b2-d440e0426c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e00a487-7f5e-41e4-a2a3-f6320a6abe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os    \n",
    "import sys\n",
    "\n",
    "APP_ROOT = os.path.abspath('..')\n",
    "sys.path.append(os.path.abspath(APP_ROOT))\n",
    "\n",
    "from chart_reasoning.pipeline_oss import ChartReasoningOSSPipeline\n",
    "\n",
    "\n",
    "vistext_data_dir = os.path.join(APP_ROOT, 'data', 'vistext-data')\n",
    "output_dir = os.path.join(APP_ROOT, 'output', 'chart-reasoning-trail-run')\n",
    "\n",
    "# need to move the outputs to chart-reasoning-trail-run/03-oss-chart-reasoning-output, .e.g, phi-3.5-vision\n",
    "model_name = 'phi-3.5-vision'\n",
    "pipeline = ChartReasoningOSSPipeline(vistext_data_dir, output_dir, model_name=model_name)\n",
    "\n",
    "# ensure the Pipeline arguments are the same with the ones in the chart-reasoning-pipeline.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af73275a-6db5-4b24-b577-435162d85a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.task_generation(sample_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694f16b6-0959-43b0-81d6-4cafe40e6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.graded_reasoning_output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e9f44bd-4251-4677-8831-c88d22286b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15302/15302 [02:32<00:00, 100.15it/s]\n",
      "100%|██████████| 15302/15302 [02:10<00:00, 116.99it/s] \n",
      "100%|██████████| 15302/15302 [02:43<00:00, 93.86it/s]  \n",
      "100%|██████████| 15302/15302 [03:05<00:00, 82.37it/s]  \n",
      "100%|██████████| 15302/15302 [02:01<00:00, 126.15it/s] \n",
      "100%|██████████| 15302/15302 [01:53<00:00, 135.33it/s] \n",
      "100%|██████████| 15302/15302 [02:13<00:00, 114.55it/s] \n",
      "100%|██████████| 15302/15302 [02:02<00:00, 125.07it/s] \n",
      "100%|██████████| 15302/15302 [02:05<00:00, 121.69it/s] \n",
      "100%|██████████| 15302/15302 [01:43<00:00, 148.24it/s] \n"
     ]
    }
   ],
   "source": [
    "for i in range(10):  # only grading is needed for open-source models\n",
    "    pipeline.grade_with_text_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323acc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8741 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8741/8741 [47:48<00:00,  3.05it/s]  \n"
     ]
    }
   ],
   "source": [
    "pipeline.graded_output_to_md()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5ae33a",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "from copy import deepcopy\n",
    "\n",
    "# code_graded_dir = os.path.join(output_dir, '06-code-assistant-grading-output')\n",
    "text_graded_dir = os.path.join(output_dir, '04-oss-text-grading-output', model_name)\n",
    "\n",
    "# RQ2 exp1\n",
    "# chart_types = ['line', 'scatter', 'bar']\n",
    "# RQ2 exp2\n",
    "# chart_types = ['pie', 'table', 'bar_anno', 'line_anno', 'scatter_anno']\n",
    "\n",
    "# RQ1 exp1\n",
    "# chart_types = ['unaligned_rule', 'color', 'size', 'scatter']\n",
    "# RQ1 exp2\n",
    "chart_types = ['rule', 'scatter_size', 'bar', 'bar_color']\n",
    "\n",
    "text_student_answer_correctness_dict_by_chart_type = dict()\n",
    "text_student_judgement_list = dict()\n",
    "for chart_type in chart_types:\n",
    "    text_student_answer_correctness_dict_by_chart_type[chart_type] = dict()\n",
    "    text_student_judgement_list[chart_type] = []\n",
    "\n",
    "graded_task_ids = open(\"reported_ids_1000.txt\", \"r\").readlines()\n",
    "graded_task_ids = [one_id.strip() for one_id in graded_task_ids]\n",
    "\n",
    "valid_task_ids = list(set(graded_task_ids))\n",
    "for task_id in tqdm(valid_task_ids):\n",
    "    # text_graded_task_file = os.path.join(text_graded_dir, f'{task_id}.grade.json')\n",
    "    for chart_type in chart_types:\n",
    "        text_graded_task_file = os.path.join(text_graded_dir, f'{task_id}.{chart_type}.grade.json')\n",
    "\n",
    "        if os.path.exists(text_graded_task_file):\n",
    "            with open(text_graded_task_file, 'r') as f:\n",
    "                text_graded_task = json.load(f)\n",
    "                # print(\"len(text_graded_task): \", len(text_graded_task))\n",
    "                for question_index, question_dict in enumerate(text_graded_task):\n",
    "                    # print(question_dict)\n",
    "                    # break\n",
    "                    try:\n",
    "                        text_student_judgement_list[chart_type].append(question_dict['student_answer_correctness'].lower())\n",
    "                    except:\n",
    "                        question_dict['student_answer_correctness'] = 'skipped'\n",
    "                    if question_dict['student_answer_correctness'] not in text_student_answer_correctness_dict_by_chart_type[chart_type].keys():\n",
    "                        text_student_answer_correctness_dict_by_chart_type[chart_type][question_dict['student_answer_correctness'].lower()] = []\n",
    "                    else:\n",
    "                        text_student_answer_correctness_dict_by_chart_type[chart_type][question_dict['student_answer_correctness'].lower()].append((question_dict['task_id']+\"_\"+str(question_index), question_dict['task_type']))\n",
    "        else:\n",
    "            print(f\"File not found: {text_graded_task_file}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort valid_task_ids\n",
    "valid_task_ids = sorted([int(task_id) for task_id in valid_task_ids])\n",
    "len(valid_task_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386cc4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_list = []\n",
    "for chart_type in chart_types:\n",
    "    print(f\"Chart Type: {chart_type}\")\n",
    "    print(\"Correctness Distribution:\")\n",
    "    total = sum([len(id_list) for id_list in text_student_answer_correctness_dict_by_chart_type[chart_type].values()])\n",
    "    for key, id_list in text_student_answer_correctness_dict_by_chart_type[chart_type].items():\n",
    "        print(key, \":\", len(id_list), f\"({round(len(id_list)/total * 100, 2)}%)\")\n",
    "        if key == 'correct':\n",
    "            correct_list.append(round(len(id_list)/total * 100, 2))\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "print(\"Correctness:\", correct_list)\n",
    "\n",
    "# total = sum([len(id_list) for id_list in text_student_answer_correctness.values()])\n",
    "# # get the distribution of correctness\n",
    "# for key, id_list in text_student_answer_correctness.items():\n",
    "#     print(key, \":\", len(id_list), f\"({round(len(id_list)/total * 100, 2)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18374987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis in terms of task type\n",
    "\n",
    "task_types = ['Find Anomalies', 'Find Correlation', 'Determine Range', 'Order', 'Filter', 'Compute Derived Value', 'Find Extremum', 'Retrieve Value', 'Find Clusters', 'Characterize Distribution']\n",
    "\n",
    "\n",
    "for task_type in task_types:\n",
    "    print(f\"Task Type: {task_type}\")\n",
    "    # only consider the correctness of the task type\n",
    "    for chart_type in chart_types:\n",
    "        # print(f\"Chart Type: {chart_type}\")\n",
    "        # get all examples for this task type for this chart and calculate the correctness\n",
    "        # total = len(text_student_answer_correctness_dict_by_chart_type[chart_type][task_type])\n",
    "        correct_cnt = 0\n",
    "        for example in text_student_answer_correctness_dict_by_chart_type[chart_type]['correct']:\n",
    "            if example[1] == task_type:\n",
    "                correct_cnt += 1\n",
    "        # count all\n",
    "        all_example_cnt = 0\n",
    "        for score_type, example_list in text_student_answer_correctness_dict_by_chart_type[chart_type].items():\n",
    "            for example in example_list:\n",
    "                if example[1] == task_type:\n",
    "                    all_example_cnt += 1\n",
    "\n",
    "        # print(\"Correctness:\", correct_cnt, \"All examples:\", all_example_cnt, f\"({round(correct_cnt/all_example_cnt * 100, 2)}%)\")\n",
    "        print(f\"{round(correct_cnt/all_example_cnt * 100, 2)}\", end=';')\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
